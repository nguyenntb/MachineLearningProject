---
title: "Training a Prediction Model on the Weight Lifting Exercise Dataset"
author: "Nguyen Nguyen"
date: "8/4/2019"
output: html_document
---

## Executive Summary

After cleaning the data and applying the PCA technique, we train our model
with 3 methods: LDA, Random Forest, and Boosting with Trees. The Random 
Forest is proved to be the best model with 100% accuracy in-sample and 98%
accuracy out-of-sample.

## Data Preprocessing

We first load data into R.

``` {r}
training <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")
```

For the cross-validation purpose, we split the original training data set 
into new training and testing data sets.

``` {r, message = FALSE, warning = FALSE}
library(caret)
set.seed(456)
inTrain <- createDataPartition(training$classe, p = 0.7, list = F)
train <- training[inTrain, ]
test <- training[-inTrain, ]
```

The training data set has `r nrow(training)` observations and `r ncol(training)` 
variables. This is a fairly large number of predictors, so we would try to reduce 
the number of predictors by applying the Principal Component Analysis (PCA). But 
before doing that, we have to clean the data.


Our outcome is the "classe" variable, which is the last column (#160) in the training data set. It measures how well an object performs the exercises. Here is the break-dowm
of the outcome "classe" in the training data set.

``` {r}
table(train$classe)
```

After taking an initial look at the training data, we found that there are a lot 
of columns with NA values and missing values. It means that
these columns provide very little information and can be removed from the training 
data set. 

Having said that, let's starting with cleaning NA's values. Note that all of these cleaning steps should be applied to both training and testing data set.

``` {r}
## Compute the number of NAs in each column
naCompute <- sapply(train[,-160], function(x) sum(is.na(x)))
## Get names of the columns with no NAs
naSelect <- naCompute[naCompute == 0]
naNames <- names(naSelect)
## Subset the columns with no NAs
trainNA <- subset(train[,-160], select = naNames)
testNA <- subset(test[,-160], select = naNames)
testingNA <- subset(testing[,-160], select = naNames)
```
Then, we remove all columns with a large proportion of missing values.

``` {r na}
## Compute the number of missing values in each column
missCompute <- sapply(trainNA, function(x) sum(x == ""))
## Get names of the columns with no missing values
missSelect <- missCompute[missCompute == 0]
missNames <- names(missSelect)
## Subset the columns with no missing values
trainMiss <- subset(trainNA, select = missNames)
testMiss <- subset(testNA, select = missNames)
testingMiss <- subset(testingNA, select = missNames)
```

The first several columns of the training data set contain only index 
information, except for the "user_name" variable, and should be removed.

``` {r miss}
trainClean <- trainMiss[,-c(1:7)]
testClean <- testMiss[,-c(1:7)]
testingClean <- testingMiss[,-c(1:7)]
```

After all of these cleaning steps, our training data set has `r ncol(trainClean)`
variables and is ready to be applied with PCA. The PCA technique would help to 
remove the highly correlated variables while retain much of the information.

```{r PCA, warning = FALSE, message = FALSE}
# calculate the pre-process parameters from the dataset
preProcess <- preProcess(trainClean, method = c("center", "scale", "pca"))
# transform the dataset using the parameters
trainPC <- predict(preProcess, trainClean)
testPC <- predict(preProcess, testClean)
testingPC <- predict(preProcess, testingClean)
```

With the PCA technique, we are able to reduce the number of predictors from 52 
to 25. Finally, we put the user_name variable and the outcome to the clean 
training and testing data sets.

``` {r}
trainFinal <- cbind(user_name = train$user_name, trainPC, classe = train$classe)
testFinal <- cbind(user_name = test$user_name, testPC, classe = test$classe)
testingFinal <- cbind(user_name = testing$user_name, testingPC)
```

## Train with different models

Because we have a huge amount of data, it would take a long time to train our
models. Therefore, we use parralel processing to improve the performance of 
the caret package.

Below is the code to configure parralel processing.

``` {r, message = FALSE, warning = FALSE}
library(parallel)
library(doParallel)
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)
```

Then, we configure the trainControl object to allow for parralel processing 
and cross validation with the k-fold method (k = 5).


``` {r}
train.control <- trainControl(method = "cv", number = 5, allowParallel = TRUE)
``` 
We would train with 3 different models: Linear Discriminant Analysis (LDA), 
Random Forest, and Boosting with trees. We will choose the model with the 
highest accuracy.

### Train with Linear Discriminant Analysis model

``` {r lda, cache = TRUE}
set.seed(123) 
ldaFit <- train(classe ~., data = trainFinal, method = "lda", 
                trControl = train.control)
ldaPredIn <- predict(ldaFit, trainFinal)
ldaPredOut <- predict(ldaFit, testFinal)
```

Let's see how this model performs in-sample,

``` {r}
table(ldaPredIn, trainFinal$classe)
```
and out-of-sample,

``` {r}
table(ldaPredOut, testFinal$classe)
```
There are a lot of off-diagonal elements in these tables, so the LDA model 
does not perform very well. The accuracy for this model is:

``` {r}
ldaAccuIn <- confusionMatrix(trainFinal$classe, ldaPredIn)$overall[1]
ldaAccuOut <- confusionMatrix(testFinal$classe, ldaPredOut)$overall[1]
cbind("In-Sample" = ldaAccuIn, "Out-of-Sample" = ldaAccuOut)
```


### Train with Random Forest

``` {r rf, cache = TRUE}
set.seed(123) 
rfFit <- train(classe ~., data = trainFinal, method = "rf",
                trControl = train.control)
rfPredIn <- predict(rfFit, trainFinal)
rfPredOut <- predict(rfFit, testFinal)
```

The in-sample prediction matrix of the random forest is:

``` {r}
table(rfPredIn, trainFinal$classe)
```

The out-of-sample prediction matrix of the random forest is:

``` {r}
table(rfPredOut, testFinal$classe)
```

As you can see, the Random Forest model does an amazing job in predicting the 
outcome in the training set with zero errors in-sample and very little off-diagnonal
elements. And we can confirm this with the accuracy of this model:

```{r}
rfAccuIn <- confusionMatrix(trainFinal$classe, rfPredIn)$overall[1]
rfAccuOut <- confusionMatrix(testFinal$classe, rfPredOut)$overall[1]
cbind("In-Sample" = rfAccuIn, "Out-of-Sample" = rfAccuOut)
```


### Train with Boosting with trees

``` {r gbm, cache = TRUE}
set.seed(123) 
gbmFit <- train(classe ~., data = trainFinal, method = "gbm", verbose = FALSE,
                trControl = train.control)
gbmPredIn <- predict(gbmFit, trainFinal)
gbmPredOut <- predict(gbmFit, testFinal)
```

Let's again look at the in-sample prediction table,

``` {r}
table(gbmPredIn, trainFinal$classe)
```

and the out-of-sample prediction table,

``` {r}
table(gbmPredOut, testFinal$classe)
```

Like the LDA model, we still see a lot of off-diagonal elements in the 
prediction matrix, which implies that the model is not trained well. 
The in-sample and out-of-sample accuracy of this model is:

``` {r}
gbmAccuIn <- confusionMatrix(trainFinal$classe, gbmPredIn)$overall[1]
gbmAccuOut <- confusionMatrix(testFinal$classe, gbmPredOut)$overall[1]
cbind("In-Sample" = gbmAccuIn, "Out-of-Sample" = gbmAccuOut)
```


After done with the training step, we should de-register parallel processing 
cluster.

``` {r}
stopCluster(cluster)
registerDoSEQ()
```

The Random Forest model gives us the best accuracy, so we will use this model 
to predict the testing data.

## Prediction with the testing data set

As we mentioned before, we will use the Random Forest model to predict with 
the testing data set. The in-sample error of this model is zero, i.e., a 
perfect prediction, so we expect the model will give a high accuracy for 
the out-of-sample prediction.

``` {r, results = "hide"}
predict(rfFit, testingFinal)
```


